# Flow Matching Training Configuration
trainer_args:
  # Flow matching specific parameters
  scheduler_type: "polynomial"
  exponent: 2.0
  loss_function: "generalized_kl"  # or "cross_entropy"
  source_distribution: "mask"  # or "uniform"
  time_epsilon: 1e-3

# Training Arguments
training_args:
  output_dir: "./llm_cosformer_results"
  overwrite_output_dir: true
  
  # Training parameters
  num_train_epochs: 50
  per_device_train_batch_size: 8  # Adjust based on your GPU memory
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8  # Effective batch size = 8 * 8 = 64
  max_steps: 1000000000
  
  # Learning rate and optimization
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # Evaluation and saving
  eval_strategy: "steps"
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Logging
  logging_steps: 100
  logging_dir: "./logs"
  report_to: null  # Disable wandb/tensorboard by default
  
  # Memory optimization
  fp16: false
  gradient_checkpointing: false  # Disabled for flow matching
  dataloader_pin_memory: true
  dataloader_num_workers: 2
  
  # Other settings
  remove_unused_columns: false  # Important for flow matching
  prediction_loss_only: true

# Model Configuration
model_config:
  # Core model parameters
  vocab_size: 32768
  hidden_size: 512
  intermediate_size: 1024
  num_hidden_layers: 6  # Number of transformer blocks for flow matching
  num_attention_heads: 8
  num_key_value_heads: 8
  max_position_embeddings: 4096
  attention_dropout: 0.1
  
  # Flow matching specific parameters
  timestep_emb_dim: 256
  mlp_ratio: 4

# Dataset Configuration
dataset_config:
  tokenizer_path: "Tokenizer_32768_v1"
  # For HuggingFace datasets:
  # dataset_name: "stanfordnlp/imdb"
  # For local datasets (uncomment and modify path):
  dataset_name: "/Users/liuxiaoyan/Documents/papers/CosFormer-FlowMatching/stanfordnlp/imdb"
  # dataset_name: "./my_local_dataset"  # Alternative local path
  chunk_size: 512
  max_train_samples: null  # null for all samples, or set a number to limit
  max_eval_samples: 1000