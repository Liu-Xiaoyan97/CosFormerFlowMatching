training_args:
  output_dir: "./llm_cosformer_results"
  overwrite_output_dir: true
  num_train_epochs: 3
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  warmup_steps: 300
  learning_rate: 1e-4
  lr_scheduler_type: "cosine"
  logging_steps: 10
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  data_seed: 1
  ignore_data_skip: true
  prediction_loss_only: true
  remove_unused_columns: false
  dataloader_pin_memory: false
  use_cpu: true
  save_steps: 100
  eval_steps: 100
  eval_strategy: "steps"